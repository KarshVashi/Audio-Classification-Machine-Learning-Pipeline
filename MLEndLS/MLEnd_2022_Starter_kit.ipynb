{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvrRvdTwxnvV"
      },
      "source": [
        "# Environment set up\n",
        "\n",
        "In this section we will set up a Colab environment for the MLEnd mini-project. Before starting, follow these simple instructions: \n",
        "\n",
        "1.   Go to https://drive.google.com/\n",
        "2.   Create a folder named 'Data' in 'MyDrive': On the left, click 'New' > 'Folder', enter the name 'Data', and click 'create'\n",
        "3.   Open the 'Data' folder and create a folder named 'MLEndLS'.\n",
        "\n",
        "Let's start by loading a few useful Python libraries and mounting our personal Google Drive storage system (i.e. making it available, so that Colab can access it)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gjrwd7Cxez5"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os, sys, re, pickle, glob\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "import IPython.display as ipd\n",
        "from tqdm import tqdm\n",
        "import librosa\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfJ0Fm_ey_2f"
      },
      "source": [
        "# Data download\n",
        "\n",
        "In this section we will download a small subsample of the MLEnd London Sounds Dataset. \n",
        "\n",
        "You should be able to download the entire training dataset when it is made available using a similar approach to the one used here for the small sample. As you will see, you only need to provide a different link. Please note that even though we call it \"training\" dataset you can do whatever you want with it (for instance validate a set of models). We have kept a separate private dataset for testing purposes, which we won't share with anyone.\n",
        "\n",
        "First, let's define a function that will allow us to download a file into a chosen location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPYQeCyEzKpK"
      },
      "source": [
        "def download_url(url, save_path):\n",
        "    with urllib.request.urlopen(url) as dl_file:\n",
        "        with open(save_path, 'wb') as out_file:\n",
        "            out_file.write(dl_file.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnZtCTVGzPuc"
      },
      "source": [
        "The next step is to download the file 'MLEndLS_Sample.zip' into the folder 'MyDrive/Data/MLEndLS'. Note that this might take a while!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pRFkQinzRtK"
      },
      "source": [
        "url  = \"https://collect.qmul.ac.uk/down?t=610I73D2G2RLS4BI/R8RDLM2R3O9FHEMR92GKGR0\"\n",
        "save_path = '/content/drive/MyDrive/Data/MLEndLS/MLEndLS_Sample.zip'\n",
        "download_url(url, save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fddX9WV_0oWx"
      },
      "source": [
        "Run the following cell to check that the MLEndLS folder contains the file 'MLEndLS_Sample.zip':"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mroJNE2m0nI9"
      },
      "source": [
        "path = '/content/drive/MyDrive/Data/MLEndLS'\n",
        "os.listdir(path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD2rP-8AzWS4"
      },
      "source": [
        "# Understanding our dataset sample\n",
        "\n",
        "Let's unzip the sample data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOPTdwA9zeyz"
      },
      "source": [
        "directory_to_extract_to = '/content/drive/MyDrive/Data/MLEndLS/sample/'\n",
        "zip_path = '/content/drive/MyDrive/Data/MLEndLS/MLEndLS_Sample.zip'\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(directory_to_extract_to)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now check how many audio files we have:"
      ],
      "metadata": {
        "id": "rdP6Mk0PriRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_path = '/content/drive/MyDrive/Data/MLEndLS/sample/MLEndLS_Sample/*.wav'\n",
        "files = glob.glob(sample_path)\n",
        "len(files)"
      ],
      "metadata": {
        "id": "BYrpz_QkLAp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0OeDvt7zedb"
      },
      "source": [
        "This figure (100) corresponds to the number of **items** or **samples** in our dataset. Let's listen to some random audio files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcqXgKzgzmwG"
      },
      "source": [
        "for _ in range(5):\n",
        "  n = np.random.randint(98)\n",
        "  display(ipd.Audio(files[n]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3qkV3Im2WET"
      },
      "source": [
        "Can you recognise the filming spot? What can you say about the acoustic environment?\n",
        "\n",
        "The MLEndLS Dataset is a collection of samples described by 5 attributes:\n",
        "- Audio\n",
        "- Area\n",
        "- Spot\n",
        "- Whether indoor or outdoor\n",
        "- Participant\n",
        "\n",
        "Upload the CSV file 'MLEndLS_Sample.csv'. Then run the next cell to load it into a Pandas DataFrame and print its contents.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MLENDLS_df = pd.read_csv('./MLEndLS_Sample.csv').set_index('file_id') \n",
        "MLENDLS_df"
      ],
      "metadata": {
        "id": "BxXrmlDt1DV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The audio attribute is a complex one and cannot be directly inserted in a DataFrame structure. Instead, we have the name of the corresponding audio files, e.g. '3086.wav', which in our case are stored in the '/content/drive/MyDrive/Data/MLEndLS/sample/MLEndLS_Sample/' folder. \n",
        "\n",
        "The next cell prints the contents of the folder that stores our sample audio files. You can see that the names of the WAV files correspond to one of the `file_id` entries in the `MLEndLS_df` DataFrame."
      ],
      "metadata": {
        "id": "UAKlreTX1ryY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHa-FqEg2Gyu"
      },
      "source": [
        "for file in files:\n",
        "  print(file.split('/')[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJVADYvuHq3x"
      },
      "source": [
        "# Feature extraction : Picth\n",
        "\n",
        "Audio files are complex data types. Specifically they are **discrete signals** or **time series**, consisting of values on a 1D temporal grid. These values are known as *samples* themselves, which might be a bit confusing, as we have used this term to refer to the *items* in our dataset. The **sampling frequency** is the rate at which samples in an audio file are produced. For instance a sampling frequency of 5HZ indicates that 5 produce 5 samples per second, or 1 sample every 0.2 s.\n",
        "\n",
        "Let's plot one of our audio signals:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU7cSdySHx8P"
      },
      "source": [
        "n=0\n",
        "fs = None # Sampling frequency\n",
        "x, fs = librosa.load(files[n],sr=fs)\n",
        "t = np.arange(len(x))/fs\n",
        "plt.plot(t,x)\n",
        "plt.xlabel('time (sec)')\n",
        "plt.ylabel('amplitude')\n",
        "plt.show()\n",
        "display(ipd.Audio(files[n]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVndEmbJIR5-"
      },
      "source": [
        "Can you tell whether you are listening to an indoors or outdoors location? Does it agree with the values shown in the ``` MLENDLS_df ``` dataframe? Let's check it:\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4cSPJxnT80j"
      },
      "source": [
        "MLENDLS_df.loc[files[n].split('/')[-1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Weh65CY7T9da"
      },
      "source": [
        "Note that we are using the name of the audio file as the index in the Pandas DataFrame. By changing the value of `n` in the previous cell, you can listen to other examples. If you are doing this during one of our lab sessions, please make sure that your mic is muted!\n",
        "\n",
        "Exactly, how complex is an audio signal? Let's start by looking at the number of samples (i.e. time series samples) in one of our audio files:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oSPjIW9JZWv"
      },
      "source": [
        "n=0\n",
        "x, fs = librosa.load(files[n],sr=fs)\n",
        "print('This audio signal has', len(x), 'samples')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrNs96siJggo"
      },
      "source": [
        "If we are using a raw audio signal as the input of a machine learning model, we will be operating in a predictor space consisting of hundreds of thousands of dimensions. Compare this figure with the number of items (i.e. recordings) that we have. Do we have enough samples to train a model that takes one of these audio signals as an input?\n",
        "\n",
        "One approach to deal with this huge dimensionality is to extract a few features from our signals and use these features as predictors instead. In this notebook we will use four audio features, namely:\n",
        "\n",
        "\n",
        "1.   Power.\n",
        "2.   Pitch - mean.\n",
        "3.   Pitch - standard deviation.\n",
        "4.   Fraction of voiced region.\n",
        "\n",
        "In the next cell, we define a new function that gets the pitch of an audio signal (do not worry if you do not know what it is, but feel free to read about it!)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiXYO1tdJgIs"
      },
      "source": [
        "def getPitch(x,fs,winLen=0.02):\n",
        "  #winLen = 0.02 \n",
        "  p = winLen*fs\n",
        "  frame_length = int(2**int(p-1).bit_length())\n",
        "  hop_length = frame_length//2\n",
        "  f0, voiced_flag, voiced_probs = librosa.pyin(y=x, fmin=80, fmax=450, sr=fs,\n",
        "                                                 frame_length=frame_length,hop_length=hop_length)\n",
        "  return f0,voiced_flag"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCRRi_NFM3kw"
      },
      "source": [
        "Let's consider the problem of determining whether the filming spot is indoors or outdoors. Then next cell defines a function that takes a collection of audio files together with a CSV file and creates a NumPy array containing the 4 audio features used as predictors (`X`) and a binary label (`y`) that indicates whether the recording is indoors (`y=1`) or outdoors (`y=0`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEMwPrVmMHZa"
      },
      "source": [
        "def getXy(files,labels_file, scale_audio=False, onlySingleDigit=False):\n",
        "  X,y =[],[]\n",
        "  for file in tqdm(files):\n",
        "    fileID = file.split('/')[-1]\n",
        "    file_name = file.split('/')[-1]\n",
        "    yi = labels_file.loc[fileID]['in_out']=='indoor'\n",
        "\n",
        "    fs = None # if None, fs would be 22050\n",
        "    x, fs = librosa.load(file,sr=fs)\n",
        "    if scale_audio: x = x/np.max(np.abs(x))\n",
        "    f0, voiced_flag = getPitch(x,fs,winLen=0.02)\n",
        "      \n",
        "    power = np.sum(x**2)/len(x)\n",
        "    pitch_mean = np.nanmean(f0) if np.mean(np.isnan(f0))<1 else 0\n",
        "    pitch_std  = np.nanstd(f0) if np.mean(np.isnan(f0))<1 else 0\n",
        "    voiced_fr = np.mean(voiced_flag)\n",
        "\n",
        "    xi = [power,pitch_mean,pitch_std,voiced_fr]\n",
        "    X.append(xi)\n",
        "    y.append(yi)\n",
        "\n",
        "  return np.array(X),np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQdqy2ksNCD7"
      },
      "source": [
        "Let's apply `getXy` to the subsample and obtain the NumPy predictor array (`X`) and a binary label (`y`). This could take a while, as we are processing each of the 100 audio signals. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkjX_Zu9NNUs"
      },
      "source": [
        "X,y = getXy(files, labels_file=MLENDLS_df, scale_audio=True, onlySingleDigit=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOA190Z0kSVq"
      },
      "source": [
        "The next cell shows the shape of `X` and `y` and prints the labels vector `y`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcDtFySakTeP"
      },
      "source": [
        "print('The shape of X is', X.shape) \n",
        "print('The shape of y is', y.shape)\n",
        "print('The labels vector is', y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVvbOCDBZbQx"
      },
      "source": [
        "As you can see, we have 100 items consisting of 4 features (stored in `X`) and one binary label (stored in `y`). Is our dataset balanced? Let's have a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tW_grfnuYmjD"
      },
      "source": [
        "print(' The number of indoor recordings is ', np.count_nonzero(y))\n",
        "print(' The number of outdoor recordings is ', y.size - np.count_nonzero(y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp1Kgp5OkdC-"
      },
      "source": [
        "# Modeling: Support Vector Machines\n",
        "\n",
        "Let's build a support vector machine (SVM) model for the predictive task of identifying the type of filming spot (indoors/outdoors) using the dataset that we have just created.\n",
        "\n",
        "We will use the SVM method provided by scikit-learn and will split the dataset defined by X and y into a training set and a validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uv0rBpWFkoe6"
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.3)\n",
        "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqPeEFEqk_cS"
      },
      "source": [
        "Can you identify the number of items in the training and validation sets?\n",
        "\n",
        "Let's now fit an SVM model and print both the training accuracty and validation accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZYHghqhlDeM"
      },
      "source": [
        "model  = svm.SVC(C=1)\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "yt_p = model.predict(X_train)\n",
        "yv_p = model.predict(X_val)\n",
        "\n",
        "print('Training Accuracy', np.mean(yt_p==y_train))\n",
        "print('Validation  Accuracy', np.mean(yv_p==y_val))\n",
        "print('The support vectors are', model.support_vectors_.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFr71A-blKsN"
      },
      "source": [
        "Compare the training and validation accuracies. Is our model overfitting, underfitting, performing well? What do you think the accuracy of a random classifier would be?\n",
        "\n",
        "Let's normalise the predictors, to see if the performance improves.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJGHrtdWlLre"
      },
      "source": [
        "mean = X_train.mean(0)\n",
        "sd =  X_train.std(0)\n",
        "\n",
        "X_train = (X_train-mean)/sd\n",
        "X_val  = (X_val-mean)/sd\n",
        "\n",
        "model  = svm.SVC(C=1,gamma=2)\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "yt_p = model.predict(X_train)\n",
        "yv_p = model.predict(X_val)\n",
        "\n",
        "print('Training Accuracy', np.mean(yt_p==y_train))\n",
        "print('Validation  Accuracy', np.mean(yv_p==y_val))\n",
        "print('The support vectors are', model.support_vectors_.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMHpZlB5lPr_"
      },
      "source": [
        "Do you think we have obtained a better solution?\n",
        "\n",
        "What machine learning pipeline have we implemented for each solution? Feel free to try other machine learning models available in scikit, it's very easy!"
      ]
    }
  ]
}